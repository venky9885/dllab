# -*- coding: utf-8 -*-
"""04_Visualize Training History_NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ocldzZ56vPrf8ZZKDWSXHeNrOc42nI1b
"""



"""# Visualizing the Training History

When our neural network is new, it will have a poor performance. 
As the neural network learns on the training data, the model’s error on both the training and test set will tend to increase. 
However, at a certain point the neural network starts “memorizing” the training data, and overfits. 
When this starts happening, the training error will decrease while the test error will start increasing. 
Therefore, in many cases there is a “sweet spot” where the test error is at its lowest point. 
This effect can be plainly seen in the solution where we visualize the training and test loss at each epoch. 
Note that the test error is lowest around epoch five, after which the training loss continues to increase while the test loss starts increasing. 
At this point onward, the model is overfitting.
"""

# Use Matplotlib to visualize the loss of the test and training set over each epoch:
# Load libraries
import numpy as np
from keras.datasets import imdb
from keras.preprocessing.text import Tokenizer
from keras import models
from keras import layers
import matplotlib.pyplot as plt

# Set random seed
np.random.seed(0)

# Set the number of features we want
number_of_features = 10000

# Load data and target vector from movie review data
(data_train, target_train), (data_test, target_test) = imdb.load_data(num_words=number_of_features)

# Convert movie review data to a one-hot encoded feature matrix
tokenizer = Tokenizer(num_words=number_of_features)
features_train = tokenizer.sequences_to_matrix(data_train, mode="binary")
features_test = tokenizer.sequences_to_matrix(data_test, mode="binary")

# Start neural network
network = models.Sequential()

# Add fully connected layer with a ReLU activation function
network.add(layers.Dense(units=16,
activation="relu",
input_shape=(number_of_features,)))

# Add fully connected layer with a ReLU activation function
network.add(layers.Dense(units=16, activation="relu"))

# Add fully connected layer with a sigmoid activation function
network.add(layers.Dense(units=1, activation="sigmoid"))

# Compile neural network
network.compile(loss="binary_crossentropy", # Cross-entropy
optimizer="rmsprop", # Root Mean Square Propagation
metrics=["accuracy"]) # Accuracy performance metric

# Train neural network
history = network.fit(features_train, # Features
target_train, # Target
epochs=15, # Number of epochs
verbose=0, # No output
batch_size=1000, # Number of observations per batch
validation_data=(features_test, target_test)) # Test data

# Get training and test loss histories
training_loss = history.history["loss"]
test_loss = history.history["val_loss"]

# Create count of the number of epochs
epoch_count = range(1, len(training_loss) + 1)

epoch_count

# Visualize loss history
plt.plot(epoch_count, training_loss, "r--")
plt.plot(epoch_count, test_loss, "b-")
plt.legend(["Training Loss", "Test Loss"])
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.show();

# Alternatively, we can use the same approach to visualize the training and test accuracy over each epoch:
# Get training and test accuracy histories

training_accuracy = history.history["accuracy"]
test_accuracy = history.history["val_accuracy"]
plt.plot(epoch_count, training_accuracy, "r--")
plt.plot(epoch_count, test_accuracy, "b-")

# Visualize accuracy history
plt.legend(["Training Accuracy", "Test Accuracy"])
plt.xlabel("Epoch")
plt.ylabel("Accuracy Score")
plt.show();

"""# Note that the test error is lowest around epoch five, after which the training loss continues to increase while the test loss starts increasing. 
# At this point onward, the model is overfitting.
"""

history

history.history